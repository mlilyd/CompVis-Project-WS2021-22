{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Image classification with vision transformer\n",
    "Source: https://keras.io/examples/vision/image_classification_with_vision_transformer/#build-the-vit-model\n",
    "\n",
    "## Introduction\n",
    "This example implements the Vision Transformer (ViT) model by Alexey Dosovitskiy et al. for image classification, and demonstrates it on the CIFAR-100 dataset (changed to CIFAR-10). The ViT model applies the Transformer architecture with self-attention to sequences of image patches, without using convolution layers.\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher, as well as TensorFlow Addons, which can be installed using the following command:\n",
    "'pip install -U tensorflow-addons'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T20:46:22.618455Z",
     "iopub.status.busy": "2022-01-16T20:46:22.618115Z",
     "iopub.status.idle": "2022-01-16T20:46:28.768771Z",
     "shell.execute_reply": "2022-01-16T20:46:28.767774Z",
     "shell.execute_reply.started": "2022-01-16T20:46:22.618364Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T20:48:51.020118Z",
     "iopub.status.busy": "2022-01-16T20:48:51.019467Z",
     "iopub.status.idle": "2022-01-16T20:48:51.234935Z",
     "shell.execute_reply": "2022-01-16T20:48:51.233657Z",
     "shell.execute_reply.started": "2022-01-16T20:48:51.020073Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('CPU')\n",
    "#assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "#config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T20:48:53.379479Z",
     "iopub.status.busy": "2022-01-16T20:48:53.378722Z",
     "iopub.status.idle": "2022-01-16T20:49:01.837753Z",
     "shell.execute_reply": "2022-01-16T20:49:01.836713Z",
     "shell.execute_reply.started": "2022-01-16T20:48:53.379444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T21:48:03.479371Z",
     "iopub.status.busy": "2022-01-16T21:48:03.479022Z",
     "iopub.status.idle": "2022-01-16T21:48:03.488713Z",
     "shell.execute_reply": "2022-01-16T21:48:03.487683Z",
     "shell.execute_reply.started": "2022-01-16T21:48:03.479284Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 200\n",
    "image_size = 72 # original exmple uses 72, but this seems to lead to memory error  \n",
    "projection_dim = 64\n",
    "num_heads = 3\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 11\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T20:49:19.515192Z",
     "iopub.status.busy": "2022-01-16T20:49:19.51488Z",
     "iopub.status.idle": "2022-01-16T20:49:24.829348Z",
     "shell.execute_reply": "2022-01-16T20:49:24.828399Z",
     "shell.execute_reply.started": "2022-01-16T20:49:19.515158Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T20:51:46.650015Z",
     "iopub.status.busy": "2022-01-16T20:51:46.649684Z",
     "iopub.status.idle": "2022-01-16T20:51:46.657047Z",
     "shell.execute_reply": "2022-01-16T20:51:46.655712Z",
     "shell.execute_reply.started": "2022-01-16T20:51:46.649981Z"
    }
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the ViT model\n",
    "The ViT model consists of multiple Transformer blocks, which use the layers.MultiHeadAttention layer as a self-attention mechanism applied to the sequence of patches. The Transformer blocks produce a [batch_size, num_patches, projection_dim] tensor, which is processed via an classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the paper, which prepends a learnable embedding to the sequence of encoded patches to serve as the image representation, all the outputs of the final Transformer block are reshaped with layers.Flatten() and used as the image representation input to the classifier head. Note that the layers.GlobalAveragePooling1D layer could also be used instead to aggregate the outputs of the Transformer block, especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T20:51:49.863485Z",
     "iopub.status.busy": "2022-01-16T20:51:49.863133Z",
     "iopub.status.idle": "2022-01-16T20:51:49.895339Z",
     "shell.execute_reply": "2022-01-16T20:51:49.892317Z",
     "shell.execute_reply.started": "2022-01-16T20:51:49.863442Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vitc_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Create patches.\n",
    "    #patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    #encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    '''\n",
    "    Here we'd want to use a convolutional layer instead of patches\n",
    "    to get the encoded image?\n",
    "    '''\n",
    "    cnn_stem = keras.Sequential()\n",
    "    # 4 convolutional layers replacing the patchify layer\n",
    "    # using filter sizes [24, 48, 96, 192]\n",
    "    cnn_stem.add(layers.Conv2D(24, 2, strides=(2,2)))\n",
    "    cnn_stem.add(layers.BatchNormalization())\n",
    "    cnn_stem.add(layers.ReLU())\n",
    "    cnn_stem.add(layers.Conv2D(48, 2, strides=(2,2)))\n",
    "    cnn_stem.add(layers.BatchNormalization())\n",
    "    cnn_stem.add(layers.ReLU())\n",
    "    cnn_stem.add(layers.Conv2D(96, 2, strides=(2,2)))\n",
    "    cnn_stem.add(layers.BatchNormalization())\n",
    "    cnn_stem.add(layers.ReLU())\n",
    "    cnn_stem.add(layers.Conv2D(192, 2, strides=(2,2)))\n",
    "    cnn_stem.add(layers.BatchNormalization())\n",
    "    cnn_stem.add(layers.ReLU())\n",
    "    \n",
    "    cnn_stem.add(layers.Conv2D(projection_dim, 1, strides=(1,1)))\n",
    "    #cnn_stem.add(layers.Embedding(input_dim = 384, output_dim = 64))\n",
    "    \n",
    "    \n",
    "    encoded_cnn = cnn_stem(augmented)\n",
    "    #encoded_cnn = tf.reshape(encoded_cnn, (-1,192, 64))\n",
    "    \n",
    "    '''\n",
    "    The part below for the transformer block is unchanged\n",
    "    '''\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_cnn)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_cnn])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_cnn = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_cnn)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T21:48:09.200102Z",
     "iopub.status.busy": "2022-01-16T21:48:09.199764Z",
     "iopub.status.idle": "2022-01-16T22:15:18.179449Z",
     "shell.execute_reply": "2022-01-16T22:15:18.17844Z",
     "shell.execute_reply.started": "2022-01-16T21:48:09.200061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "157/157 [==============================] - 76s 431ms/step - loss: 1.8770 - accuracy: 0.3263 - top-5-accuracy: 0.8313 - val_loss: 2.0239 - val_accuracy: 0.2931 - val_top-5-accuracy: 0.7806\n",
      "Epoch 2/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 1.5337 - accuracy: 0.4408 - top-5-accuracy: 0.9088 - val_loss: 1.4324 - val_accuracy: 0.4786 - val_top-5-accuracy: 0.9222\n",
      "Epoch 3/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 1.4159 - accuracy: 0.4895 - top-5-accuracy: 0.9246 - val_loss: 1.2805 - val_accuracy: 0.5512 - val_top-5-accuracy: 0.9425\n",
      "Epoch 4/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 1.3362 - accuracy: 0.5217 - top-5-accuracy: 0.9364 - val_loss: 1.1908 - val_accuracy: 0.5753 - val_top-5-accuracy: 0.9527\n",
      "Epoch 5/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 1.2521 - accuracy: 0.5544 - top-5-accuracy: 0.9444 - val_loss: 1.1249 - val_accuracy: 0.6015 - val_top-5-accuracy: 0.9577\n",
      "Epoch 6/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 1.1997 - accuracy: 0.5750 - top-5-accuracy: 0.9493 - val_loss: 1.0888 - val_accuracy: 0.6213 - val_top-5-accuracy: 0.9641\n",
      "Epoch 7/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 1.1568 - accuracy: 0.5917 - top-5-accuracy: 0.9544 - val_loss: 1.0263 - val_accuracy: 0.6382 - val_top-5-accuracy: 0.9625\n",
      "Epoch 8/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 1.1007 - accuracy: 0.6111 - top-5-accuracy: 0.9579 - val_loss: 1.0288 - val_accuracy: 0.6377 - val_top-5-accuracy: 0.9691\n",
      "Epoch 9/200\n",
      "157/157 [==============================] - 67s 428ms/step - loss: 1.0783 - accuracy: 0.6230 - top-5-accuracy: 0.9605 - val_loss: 1.0219 - val_accuracy: 0.6444 - val_top-5-accuracy: 0.9650\n",
      "Epoch 10/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 1.0461 - accuracy: 0.6300 - top-5-accuracy: 0.9631 - val_loss: 1.0984 - val_accuracy: 0.6215 - val_top-5-accuracy: 0.9616\n",
      "Epoch 11/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 1.0097 - accuracy: 0.6414 - top-5-accuracy: 0.9668 - val_loss: 0.9334 - val_accuracy: 0.6721 - val_top-5-accuracy: 0.9717\n",
      "Epoch 12/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.9808 - accuracy: 0.6577 - top-5-accuracy: 0.9676 - val_loss: 1.1125 - val_accuracy: 0.6259 - val_top-5-accuracy: 0.9585\n",
      "Epoch 13/200\n",
      "157/157 [==============================] - 67s 428ms/step - loss: 0.9572 - accuracy: 0.6620 - top-5-accuracy: 0.9701 - val_loss: 0.8731 - val_accuracy: 0.6966 - val_top-5-accuracy: 0.9756\n",
      "Epoch 14/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.9288 - accuracy: 0.6706 - top-5-accuracy: 0.9729 - val_loss: 0.8449 - val_accuracy: 0.7053 - val_top-5-accuracy: 0.9767\n",
      "Epoch 15/200\n",
      "157/157 [==============================] - 67s 428ms/step - loss: 0.9105 - accuracy: 0.6783 - top-5-accuracy: 0.9743 - val_loss: 0.8523 - val_accuracy: 0.7082 - val_top-5-accuracy: 0.9788\n",
      "Epoch 16/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.8853 - accuracy: 0.6891 - top-5-accuracy: 0.9744 - val_loss: 0.8435 - val_accuracy: 0.7100 - val_top-5-accuracy: 0.9761\n",
      "Epoch 17/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.8589 - accuracy: 0.6955 - top-5-accuracy: 0.9774 - val_loss: 0.7667 - val_accuracy: 0.7296 - val_top-5-accuracy: 0.9809\n",
      "Epoch 18/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.8409 - accuracy: 0.7049 - top-5-accuracy: 0.9785 - val_loss: 0.7763 - val_accuracy: 0.7276 - val_top-5-accuracy: 0.9813\n",
      "Epoch 19/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.8103 - accuracy: 0.7149 - top-5-accuracy: 0.9796 - val_loss: 0.7397 - val_accuracy: 0.7425 - val_top-5-accuracy: 0.9822\n",
      "Epoch 20/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.7886 - accuracy: 0.7244 - top-5-accuracy: 0.9811 - val_loss: 0.7205 - val_accuracy: 0.7525 - val_top-5-accuracy: 0.9858\n",
      "Epoch 21/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.7776 - accuracy: 0.7266 - top-5-accuracy: 0.9813 - val_loss: 0.6743 - val_accuracy: 0.7642 - val_top-5-accuracy: 0.9859\n",
      "Epoch 22/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.7505 - accuracy: 0.7358 - top-5-accuracy: 0.9832 - val_loss: 0.7167 - val_accuracy: 0.7528 - val_top-5-accuracy: 0.9845\n",
      "Epoch 23/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.7435 - accuracy: 0.7406 - top-5-accuracy: 0.9827 - val_loss: 0.6602 - val_accuracy: 0.7712 - val_top-5-accuracy: 0.9885\n",
      "Epoch 24/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.7293 - accuracy: 0.7439 - top-5-accuracy: 0.9841 - val_loss: 0.6905 - val_accuracy: 0.7605 - val_top-5-accuracy: 0.9848\n",
      "Epoch 25/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.7065 - accuracy: 0.7505 - top-5-accuracy: 0.9854 - val_loss: 0.6621 - val_accuracy: 0.7705 - val_top-5-accuracy: 0.9867\n",
      "Epoch 26/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.7030 - accuracy: 0.7539 - top-5-accuracy: 0.9858 - val_loss: 0.6219 - val_accuracy: 0.7821 - val_top-5-accuracy: 0.9894\n",
      "Epoch 27/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.6931 - accuracy: 0.7585 - top-5-accuracy: 0.9855 - val_loss: 0.6698 - val_accuracy: 0.7701 - val_top-5-accuracy: 0.9875\n",
      "Epoch 28/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.6778 - accuracy: 0.7652 - top-5-accuracy: 0.9871 - val_loss: 0.6227 - val_accuracy: 0.7828 - val_top-5-accuracy: 0.9882\n",
      "Epoch 29/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.6661 - accuracy: 0.7653 - top-5-accuracy: 0.9877 - val_loss: 0.6255 - val_accuracy: 0.7829 - val_top-5-accuracy: 0.9862\n",
      "Epoch 30/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.6699 - accuracy: 0.7640 - top-5-accuracy: 0.9868 - val_loss: 0.6409 - val_accuracy: 0.7777 - val_top-5-accuracy: 0.9882\n",
      "Epoch 31/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.6505 - accuracy: 0.7713 - top-5-accuracy: 0.9887 - val_loss: 0.5910 - val_accuracy: 0.7955 - val_top-5-accuracy: 0.9883\n",
      "Epoch 32/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.6483 - accuracy: 0.7720 - top-5-accuracy: 0.9870 - val_loss: 0.5924 - val_accuracy: 0.7933 - val_top-5-accuracy: 0.9893\n",
      "Epoch 33/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.6281 - accuracy: 0.7790 - top-5-accuracy: 0.9887 - val_loss: 0.5835 - val_accuracy: 0.7993 - val_top-5-accuracy: 0.9894\n",
      "Epoch 34/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.6281 - accuracy: 0.7807 - top-5-accuracy: 0.9886 - val_loss: 0.6034 - val_accuracy: 0.7924 - val_top-5-accuracy: 0.9885\n",
      "Epoch 35/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.6173 - accuracy: 0.7828 - top-5-accuracy: 0.9887 - val_loss: 0.5811 - val_accuracy: 0.7969 - val_top-5-accuracy: 0.9899\n",
      "Epoch 36/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.6157 - accuracy: 0.7827 - top-5-accuracy: 0.9895 - val_loss: 0.6016 - val_accuracy: 0.7962 - val_top-5-accuracy: 0.9886\n",
      "Epoch 37/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.6007 - accuracy: 0.7887 - top-5-accuracy: 0.9895 - val_loss: 0.5769 - val_accuracy: 0.8008 - val_top-5-accuracy: 0.9894\n",
      "Epoch 38/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.6017 - accuracy: 0.7923 - top-5-accuracy: 0.9900 - val_loss: 0.5823 - val_accuracy: 0.7995 - val_top-5-accuracy: 0.9898\n",
      "Epoch 39/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5911 - accuracy: 0.7942 - top-5-accuracy: 0.9900 - val_loss: 0.6127 - val_accuracy: 0.7902 - val_top-5-accuracy: 0.9877\n",
      "Epoch 40/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5986 - accuracy: 0.7904 - top-5-accuracy: 0.9900 - val_loss: 0.5987 - val_accuracy: 0.7947 - val_top-5-accuracy: 0.9881\n",
      "Epoch 41/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5807 - accuracy: 0.7974 - top-5-accuracy: 0.9901 - val_loss: 0.5784 - val_accuracy: 0.8015 - val_top-5-accuracy: 0.9898\n",
      "Epoch 42/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5722 - accuracy: 0.7970 - top-5-accuracy: 0.9911 - val_loss: 0.5849 - val_accuracy: 0.8009 - val_top-5-accuracy: 0.9894\n",
      "Epoch 43/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.5779 - accuracy: 0.7996 - top-5-accuracy: 0.9911 - val_loss: 0.5819 - val_accuracy: 0.8016 - val_top-5-accuracy: 0.9874\n",
      "Epoch 44/200\n",
      "157/157 [==============================] - 67s 428ms/step - loss: 0.5681 - accuracy: 0.8001 - top-5-accuracy: 0.9909 - val_loss: 0.5690 - val_accuracy: 0.8042 - val_top-5-accuracy: 0.9902\n",
      "Epoch 45/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.5699 - accuracy: 0.8026 - top-5-accuracy: 0.9911 - val_loss: 0.5577 - val_accuracy: 0.8116 - val_top-5-accuracy: 0.9902\n",
      "Epoch 46/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5680 - accuracy: 0.7998 - top-5-accuracy: 0.9910 - val_loss: 0.5652 - val_accuracy: 0.8050 - val_top-5-accuracy: 0.9906\n",
      "Epoch 47/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5459 - accuracy: 0.8069 - top-5-accuracy: 0.9924 - val_loss: 0.6493 - val_accuracy: 0.7849 - val_top-5-accuracy: 0.9854\n",
      "Epoch 48/200\n",
      "157/157 [==============================] - 67s 428ms/step - loss: 0.5519 - accuracy: 0.8043 - top-5-accuracy: 0.9915 - val_loss: 0.5513 - val_accuracy: 0.8125 - val_top-5-accuracy: 0.9905\n",
      "Epoch 49/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5507 - accuracy: 0.8051 - top-5-accuracy: 0.9921 - val_loss: 0.5459 - val_accuracy: 0.8136 - val_top-5-accuracy: 0.9901\n",
      "Epoch 50/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5460 - accuracy: 0.8087 - top-5-accuracy: 0.9923 - val_loss: 0.5563 - val_accuracy: 0.8103 - val_top-5-accuracy: 0.9901\n",
      "Epoch 51/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.5427 - accuracy: 0.8131 - top-5-accuracy: 0.9912 - val_loss: 0.5301 - val_accuracy: 0.8182 - val_top-5-accuracy: 0.9908\n",
      "Epoch 52/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5291 - accuracy: 0.8138 - top-5-accuracy: 0.9932 - val_loss: 0.5537 - val_accuracy: 0.8100 - val_top-5-accuracy: 0.9904\n",
      "Epoch 53/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5332 - accuracy: 0.8133 - top-5-accuracy: 0.9919 - val_loss: 0.5537 - val_accuracy: 0.8124 - val_top-5-accuracy: 0.9892\n",
      "Epoch 54/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5272 - accuracy: 0.8166 - top-5-accuracy: 0.9925 - val_loss: 0.5536 - val_accuracy: 0.8120 - val_top-5-accuracy: 0.9892\n",
      "Epoch 55/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5219 - accuracy: 0.8189 - top-5-accuracy: 0.9922 - val_loss: 0.5355 - val_accuracy: 0.8171 - val_top-5-accuracy: 0.9911\n",
      "Epoch 56/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5164 - accuracy: 0.8164 - top-5-accuracy: 0.9926 - val_loss: 0.5801 - val_accuracy: 0.8046 - val_top-5-accuracy: 0.9893\n",
      "Epoch 57/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5185 - accuracy: 0.8196 - top-5-accuracy: 0.9925 - val_loss: 0.5196 - val_accuracy: 0.8228 - val_top-5-accuracy: 0.9908\n",
      "Epoch 58/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5108 - accuracy: 0.8204 - top-5-accuracy: 0.9929 - val_loss: 0.5321 - val_accuracy: 0.8200 - val_top-5-accuracy: 0.9917\n",
      "Epoch 59/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.5173 - accuracy: 0.8199 - top-5-accuracy: 0.9922 - val_loss: 0.6170 - val_accuracy: 0.7894 - val_top-5-accuracy: 0.9873\n",
      "Epoch 60/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.5115 - accuracy: 0.8208 - top-5-accuracy: 0.9927 - val_loss: 0.5136 - val_accuracy: 0.8233 - val_top-5-accuracy: 0.9921\n",
      "Epoch 61/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5059 - accuracy: 0.8237 - top-5-accuracy: 0.9923 - val_loss: 0.5695 - val_accuracy: 0.8067 - val_top-5-accuracy: 0.9911\n",
      "Epoch 62/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.5017 - accuracy: 0.8231 - top-5-accuracy: 0.9935 - val_loss: 0.5192 - val_accuracy: 0.8249 - val_top-5-accuracy: 0.9916\n",
      "Epoch 63/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.5001 - accuracy: 0.8254 - top-5-accuracy: 0.9934 - val_loss: 0.5579 - val_accuracy: 0.8151 - val_top-5-accuracy: 0.9908\n",
      "Epoch 64/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4954 - accuracy: 0.8245 - top-5-accuracy: 0.9937 - val_loss: 0.5326 - val_accuracy: 0.8173 - val_top-5-accuracy: 0.9921\n",
      "Epoch 65/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4974 - accuracy: 0.8270 - top-5-accuracy: 0.9934 - val_loss: 0.5326 - val_accuracy: 0.8171 - val_top-5-accuracy: 0.9913\n",
      "Epoch 66/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4926 - accuracy: 0.8269 - top-5-accuracy: 0.9938 - val_loss: 0.5343 - val_accuracy: 0.8163 - val_top-5-accuracy: 0.9914\n",
      "Epoch 67/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4950 - accuracy: 0.8272 - top-5-accuracy: 0.9941 - val_loss: 0.5239 - val_accuracy: 0.8238 - val_top-5-accuracy: 0.9914\n",
      "Epoch 68/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4876 - accuracy: 0.8289 - top-5-accuracy: 0.9941 - val_loss: 0.5238 - val_accuracy: 0.8237 - val_top-5-accuracy: 0.9924\n",
      "Epoch 69/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4837 - accuracy: 0.8307 - top-5-accuracy: 0.9937 - val_loss: 0.5487 - val_accuracy: 0.8172 - val_top-5-accuracy: 0.9909\n",
      "Epoch 70/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4774 - accuracy: 0.8336 - top-5-accuracy: 0.9932 - val_loss: 0.5888 - val_accuracy: 0.7973 - val_top-5-accuracy: 0.9916\n",
      "Epoch 71/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.4897 - accuracy: 0.8281 - top-5-accuracy: 0.9937 - val_loss: 0.5309 - val_accuracy: 0.8255 - val_top-5-accuracy: 0.9914\n",
      "Epoch 72/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4766 - accuracy: 0.8341 - top-5-accuracy: 0.9936 - val_loss: 0.5720 - val_accuracy: 0.8137 - val_top-5-accuracy: 0.9877\n",
      "Epoch 73/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4749 - accuracy: 0.8338 - top-5-accuracy: 0.9938 - val_loss: 0.5504 - val_accuracy: 0.8157 - val_top-5-accuracy: 0.9907\n",
      "Epoch 74/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4799 - accuracy: 0.8325 - top-5-accuracy: 0.9944 - val_loss: 0.5497 - val_accuracy: 0.8160 - val_top-5-accuracy: 0.9903\n",
      "Epoch 75/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4733 - accuracy: 0.8356 - top-5-accuracy: 0.9936 - val_loss: 0.5468 - val_accuracy: 0.8147 - val_top-5-accuracy: 0.9903\n",
      "Epoch 76/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4701 - accuracy: 0.8357 - top-5-accuracy: 0.9941 - val_loss: 0.5323 - val_accuracy: 0.8181 - val_top-5-accuracy: 0.9905\n",
      "Epoch 77/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4819 - accuracy: 0.8314 - top-5-accuracy: 0.9940 - val_loss: 0.5557 - val_accuracy: 0.8184 - val_top-5-accuracy: 0.9901\n",
      "Epoch 78/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4619 - accuracy: 0.8397 - top-5-accuracy: 0.9948 - val_loss: 0.5495 - val_accuracy: 0.8120 - val_top-5-accuracy: 0.9898\n",
      "Epoch 79/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4603 - accuracy: 0.8372 - top-5-accuracy: 0.9945 - val_loss: 0.5292 - val_accuracy: 0.8247 - val_top-5-accuracy: 0.9913\n",
      "Epoch 80/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.4683 - accuracy: 0.8351 - top-5-accuracy: 0.9946 - val_loss: 0.5172 - val_accuracy: 0.8257 - val_top-5-accuracy: 0.9917\n",
      "Epoch 81/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4642 - accuracy: 0.8374 - top-5-accuracy: 0.9949 - val_loss: 0.5619 - val_accuracy: 0.8130 - val_top-5-accuracy: 0.9915\n",
      "Epoch 82/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4612 - accuracy: 0.8385 - top-5-accuracy: 0.9945 - val_loss: 0.5250 - val_accuracy: 0.8236 - val_top-5-accuracy: 0.9923\n",
      "Epoch 83/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.4559 - accuracy: 0.8395 - top-5-accuracy: 0.9950 - val_loss: 0.4957 - val_accuracy: 0.8294 - val_top-5-accuracy: 0.9921\n",
      "Epoch 84/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4640 - accuracy: 0.8388 - top-5-accuracy: 0.9946 - val_loss: 0.5176 - val_accuracy: 0.8242 - val_top-5-accuracy: 0.9919\n",
      "Epoch 85/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4612 - accuracy: 0.8359 - top-5-accuracy: 0.9941 - val_loss: 0.5173 - val_accuracy: 0.8230 - val_top-5-accuracy: 0.9907\n",
      "Epoch 86/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4473 - accuracy: 0.8440 - top-5-accuracy: 0.9952 - val_loss: 0.5157 - val_accuracy: 0.8245 - val_top-5-accuracy: 0.9916\n",
      "Epoch 87/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4525 - accuracy: 0.8404 - top-5-accuracy: 0.9952 - val_loss: 0.5050 - val_accuracy: 0.8271 - val_top-5-accuracy: 0.9927\n",
      "Epoch 88/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4608 - accuracy: 0.8393 - top-5-accuracy: 0.9948 - val_loss: 0.5547 - val_accuracy: 0.8093 - val_top-5-accuracy: 0.9908\n",
      "Epoch 89/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4582 - accuracy: 0.8392 - top-5-accuracy: 0.9948 - val_loss: 0.5106 - val_accuracy: 0.8250 - val_top-5-accuracy: 0.9927\n",
      "Epoch 90/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4540 - accuracy: 0.8410 - top-5-accuracy: 0.9955 - val_loss: 0.5385 - val_accuracy: 0.8205 - val_top-5-accuracy: 0.9930\n",
      "Epoch 91/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4448 - accuracy: 0.8458 - top-5-accuracy: 0.9954 - val_loss: 0.5034 - val_accuracy: 0.8282 - val_top-5-accuracy: 0.9916\n",
      "Epoch 92/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4419 - accuracy: 0.8463 - top-5-accuracy: 0.9948 - val_loss: 0.5111 - val_accuracy: 0.8248 - val_top-5-accuracy: 0.9907\n",
      "Epoch 93/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4506 - accuracy: 0.8425 - top-5-accuracy: 0.9948 - val_loss: 0.5349 - val_accuracy: 0.8163 - val_top-5-accuracy: 0.9911\n",
      "Epoch 94/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4421 - accuracy: 0.8454 - top-5-accuracy: 0.9946 - val_loss: 0.5320 - val_accuracy: 0.8233 - val_top-5-accuracy: 0.9914\n",
      "Epoch 95/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.4382 - accuracy: 0.8458 - top-5-accuracy: 0.9961 - val_loss: 0.4941 - val_accuracy: 0.8317 - val_top-5-accuracy: 0.9919\n",
      "Epoch 96/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4469 - accuracy: 0.8429 - top-5-accuracy: 0.9944 - val_loss: 0.5423 - val_accuracy: 0.8195 - val_top-5-accuracy: 0.9912\n",
      "Epoch 97/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4436 - accuracy: 0.8460 - top-5-accuracy: 0.9951 - val_loss: 0.5355 - val_accuracy: 0.8212 - val_top-5-accuracy: 0.9913\n",
      "Epoch 98/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4339 - accuracy: 0.8478 - top-5-accuracy: 0.9952 - val_loss: 0.5394 - val_accuracy: 0.8202 - val_top-5-accuracy: 0.9929\n",
      "Epoch 99/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4397 - accuracy: 0.8465 - top-5-accuracy: 0.9956 - val_loss: 0.5364 - val_accuracy: 0.8209 - val_top-5-accuracy: 0.9909\n",
      "Epoch 100/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4395 - accuracy: 0.8465 - top-5-accuracy: 0.9951 - val_loss: 0.6242 - val_accuracy: 0.7997 - val_top-5-accuracy: 0.9893\n",
      "Epoch 101/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4391 - accuracy: 0.8465 - top-5-accuracy: 0.9949 - val_loss: 0.5664 - val_accuracy: 0.8090 - val_top-5-accuracy: 0.9893\n",
      "Epoch 102/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4351 - accuracy: 0.8472 - top-5-accuracy: 0.9953 - val_loss: 0.5281 - val_accuracy: 0.8235 - val_top-5-accuracy: 0.9921\n",
      "Epoch 103/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4413 - accuracy: 0.8470 - top-5-accuracy: 0.9943 - val_loss: 0.5126 - val_accuracy: 0.8282 - val_top-5-accuracy: 0.9902\n",
      "Epoch 104/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4332 - accuracy: 0.8491 - top-5-accuracy: 0.9949 - val_loss: 0.5033 - val_accuracy: 0.8309 - val_top-5-accuracy: 0.9916\n",
      "Epoch 105/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4303 - accuracy: 0.8504 - top-5-accuracy: 0.9951 - val_loss: 0.5040 - val_accuracy: 0.8291 - val_top-5-accuracy: 0.9914\n",
      "Epoch 106/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4281 - accuracy: 0.8482 - top-5-accuracy: 0.9955 - val_loss: 0.5346 - val_accuracy: 0.8252 - val_top-5-accuracy: 0.9919\n",
      "Epoch 107/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4343 - accuracy: 0.8461 - top-5-accuracy: 0.9953 - val_loss: 0.5823 - val_accuracy: 0.8124 - val_top-5-accuracy: 0.9927\n",
      "Epoch 108/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4358 - accuracy: 0.8462 - top-5-accuracy: 0.9955 - val_loss: 0.4922 - val_accuracy: 0.8322 - val_top-5-accuracy: 0.9924\n",
      "Epoch 109/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.4354 - accuracy: 0.8485 - top-5-accuracy: 0.9948 - val_loss: 0.5036 - val_accuracy: 0.8352 - val_top-5-accuracy: 0.9924\n",
      "Epoch 110/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4317 - accuracy: 0.8494 - top-5-accuracy: 0.9947 - val_loss: 0.5153 - val_accuracy: 0.8258 - val_top-5-accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4300 - accuracy: 0.8524 - top-5-accuracy: 0.9949 - val_loss: 0.5135 - val_accuracy: 0.8278 - val_top-5-accuracy: 0.9906\n",
      "Epoch 112/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.4204 - accuracy: 0.8529 - top-5-accuracy: 0.9961 - val_loss: 0.4780 - val_accuracy: 0.8377 - val_top-5-accuracy: 0.9932\n",
      "Epoch 113/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4249 - accuracy: 0.8519 - top-5-accuracy: 0.9954 - val_loss: 0.5163 - val_accuracy: 0.8338 - val_top-5-accuracy: 0.9912\n",
      "Epoch 114/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4192 - accuracy: 0.8525 - top-5-accuracy: 0.9957 - val_loss: 0.5689 - val_accuracy: 0.8126 - val_top-5-accuracy: 0.9898\n",
      "Epoch 115/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4218 - accuracy: 0.8522 - top-5-accuracy: 0.9959 - val_loss: 0.5260 - val_accuracy: 0.8258 - val_top-5-accuracy: 0.9907\n",
      "Epoch 116/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4144 - accuracy: 0.8566 - top-5-accuracy: 0.9955 - val_loss: 0.5431 - val_accuracy: 0.8223 - val_top-5-accuracy: 0.9908\n",
      "Epoch 117/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4224 - accuracy: 0.8533 - top-5-accuracy: 0.9955 - val_loss: 0.5089 - val_accuracy: 0.8271 - val_top-5-accuracy: 0.9916\n",
      "Epoch 118/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4200 - accuracy: 0.8529 - top-5-accuracy: 0.9959 - val_loss: 0.5014 - val_accuracy: 0.8331 - val_top-5-accuracy: 0.9912\n",
      "Epoch 119/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4163 - accuracy: 0.8551 - top-5-accuracy: 0.9959 - val_loss: 0.5289 - val_accuracy: 0.8254 - val_top-5-accuracy: 0.9924\n",
      "Epoch 120/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4217 - accuracy: 0.8534 - top-5-accuracy: 0.9956 - val_loss: 0.5330 - val_accuracy: 0.8175 - val_top-5-accuracy: 0.9916\n",
      "Epoch 121/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4169 - accuracy: 0.8526 - top-5-accuracy: 0.9956 - val_loss: 0.5287 - val_accuracy: 0.8165 - val_top-5-accuracy: 0.9911\n",
      "Epoch 122/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4201 - accuracy: 0.8537 - top-5-accuracy: 0.9957 - val_loss: 0.5669 - val_accuracy: 0.8138 - val_top-5-accuracy: 0.9910\n",
      "Epoch 123/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4205 - accuracy: 0.8546 - top-5-accuracy: 0.9956 - val_loss: 0.4937 - val_accuracy: 0.8344 - val_top-5-accuracy: 0.9923\n",
      "Epoch 124/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4119 - accuracy: 0.8566 - top-5-accuracy: 0.9956 - val_loss: 0.5107 - val_accuracy: 0.8306 - val_top-5-accuracy: 0.9921\n",
      "Epoch 125/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4170 - accuracy: 0.8536 - top-5-accuracy: 0.9956 - val_loss: 0.4920 - val_accuracy: 0.8340 - val_top-5-accuracy: 0.9915\n",
      "Epoch 126/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4100 - accuracy: 0.8569 - top-5-accuracy: 0.9959 - val_loss: 0.5333 - val_accuracy: 0.8239 - val_top-5-accuracy: 0.9902\n",
      "Epoch 127/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4135 - accuracy: 0.8557 - top-5-accuracy: 0.9956 - val_loss: 0.4946 - val_accuracy: 0.8333 - val_top-5-accuracy: 0.9921\n",
      "Epoch 128/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4117 - accuracy: 0.8573 - top-5-accuracy: 0.9962 - val_loss: 0.5705 - val_accuracy: 0.8107 - val_top-5-accuracy: 0.9892\n",
      "Epoch 129/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4213 - accuracy: 0.8523 - top-5-accuracy: 0.9954 - val_loss: 0.4922 - val_accuracy: 0.8308 - val_top-5-accuracy: 0.9924\n",
      "Epoch 130/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4106 - accuracy: 0.8565 - top-5-accuracy: 0.9955 - val_loss: 0.4821 - val_accuracy: 0.8350 - val_top-5-accuracy: 0.9921\n",
      "Epoch 131/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4074 - accuracy: 0.8575 - top-5-accuracy: 0.9952 - val_loss: 0.5092 - val_accuracy: 0.8344 - val_top-5-accuracy: 0.9927\n",
      "Epoch 132/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4114 - accuracy: 0.8552 - top-5-accuracy: 0.9959 - val_loss: 0.5252 - val_accuracy: 0.8278 - val_top-5-accuracy: 0.9919\n",
      "Epoch 133/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4226 - accuracy: 0.8516 - top-5-accuracy: 0.9956 - val_loss: 0.5222 - val_accuracy: 0.8316 - val_top-5-accuracy: 0.9919\n",
      "Epoch 134/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4075 - accuracy: 0.8592 - top-5-accuracy: 0.9958 - val_loss: 0.4987 - val_accuracy: 0.8343 - val_top-5-accuracy: 0.9910\n",
      "Epoch 135/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4117 - accuracy: 0.8562 - top-5-accuracy: 0.9958 - val_loss: 0.5012 - val_accuracy: 0.8315 - val_top-5-accuracy: 0.9933\n",
      "Epoch 136/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4048 - accuracy: 0.8592 - top-5-accuracy: 0.9962 - val_loss: 0.5361 - val_accuracy: 0.8248 - val_top-5-accuracy: 0.9898\n",
      "Epoch 137/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4016 - accuracy: 0.8579 - top-5-accuracy: 0.9962 - val_loss: 0.5237 - val_accuracy: 0.8281 - val_top-5-accuracy: 0.9926\n",
      "Epoch 138/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.4093 - accuracy: 0.8582 - top-5-accuracy: 0.9958 - val_loss: 0.4739 - val_accuracy: 0.8402 - val_top-5-accuracy: 0.9929\n",
      "Epoch 139/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4087 - accuracy: 0.8568 - top-5-accuracy: 0.9960 - val_loss: 0.4836 - val_accuracy: 0.8377 - val_top-5-accuracy: 0.9929\n",
      "Epoch 140/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4027 - accuracy: 0.8580 - top-5-accuracy: 0.9962 - val_loss: 0.5034 - val_accuracy: 0.8353 - val_top-5-accuracy: 0.9931\n",
      "Epoch 141/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4034 - accuracy: 0.8574 - top-5-accuracy: 0.9962 - val_loss: 0.4985 - val_accuracy: 0.8343 - val_top-5-accuracy: 0.9927\n",
      "Epoch 142/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4097 - accuracy: 0.8569 - top-5-accuracy: 0.9954 - val_loss: 0.4859 - val_accuracy: 0.8385 - val_top-5-accuracy: 0.9923\n",
      "Epoch 143/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3981 - accuracy: 0.8616 - top-5-accuracy: 0.9962 - val_loss: 0.4865 - val_accuracy: 0.8351 - val_top-5-accuracy: 0.9916\n",
      "Epoch 144/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4014 - accuracy: 0.8610 - top-5-accuracy: 0.9954 - val_loss: 0.5348 - val_accuracy: 0.8194 - val_top-5-accuracy: 0.9914\n",
      "Epoch 145/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4034 - accuracy: 0.8592 - top-5-accuracy: 0.9960 - val_loss: 0.5461 - val_accuracy: 0.8208 - val_top-5-accuracy: 0.9903\n",
      "Epoch 146/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4030 - accuracy: 0.8591 - top-5-accuracy: 0.9966 - val_loss: 0.5875 - val_accuracy: 0.8124 - val_top-5-accuracy: 0.9890\n",
      "Epoch 147/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4089 - accuracy: 0.8596 - top-5-accuracy: 0.9954 - val_loss: 0.4963 - val_accuracy: 0.8328 - val_top-5-accuracy: 0.9932\n",
      "Epoch 148/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.4116 - accuracy: 0.8577 - top-5-accuracy: 0.9955 - val_loss: 0.5197 - val_accuracy: 0.8269 - val_top-5-accuracy: 0.9903\n",
      "Epoch 149/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4014 - accuracy: 0.8618 - top-5-accuracy: 0.9959 - val_loss: 0.5240 - val_accuracy: 0.8312 - val_top-5-accuracy: 0.9895\n",
      "Epoch 150/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3988 - accuracy: 0.8586 - top-5-accuracy: 0.9963 - val_loss: 0.5645 - val_accuracy: 0.8171 - val_top-5-accuracy: 0.9909\n",
      "Epoch 151/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.3955 - accuracy: 0.8618 - top-5-accuracy: 0.9957 - val_loss: 0.6127 - val_accuracy: 0.7973 - val_top-5-accuracy: 0.9888\n",
      "Epoch 152/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4066 - accuracy: 0.8583 - top-5-accuracy: 0.9958 - val_loss: 0.5093 - val_accuracy: 0.8333 - val_top-5-accuracy: 0.9916\n",
      "Epoch 153/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3950 - accuracy: 0.8620 - top-5-accuracy: 0.9963 - val_loss: 0.6584 - val_accuracy: 0.7859 - val_top-5-accuracy: 0.9876\n",
      "Epoch 154/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3998 - accuracy: 0.8608 - top-5-accuracy: 0.9962 - val_loss: 0.5387 - val_accuracy: 0.8232 - val_top-5-accuracy: 0.9896\n",
      "Epoch 155/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3991 - accuracy: 0.8604 - top-5-accuracy: 0.9961 - val_loss: 0.5370 - val_accuracy: 0.8240 - val_top-5-accuracy: 0.9919\n",
      "Epoch 156/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3976 - accuracy: 0.8609 - top-5-accuracy: 0.9956 - val_loss: 0.5411 - val_accuracy: 0.8185 - val_top-5-accuracy: 0.9900\n",
      "Epoch 157/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3959 - accuracy: 0.8605 - top-5-accuracy: 0.9959 - val_loss: 0.5715 - val_accuracy: 0.8108 - val_top-5-accuracy: 0.9880\n",
      "Epoch 158/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3842 - accuracy: 0.8673 - top-5-accuracy: 0.9959 - val_loss: 0.5454 - val_accuracy: 0.8219 - val_top-5-accuracy: 0.9910\n",
      "Epoch 159/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.3945 - accuracy: 0.8628 - top-5-accuracy: 0.9966 - val_loss: 0.4881 - val_accuracy: 0.8410 - val_top-5-accuracy: 0.9917\n",
      "Epoch 160/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3924 - accuracy: 0.8635 - top-5-accuracy: 0.9965 - val_loss: 0.5238 - val_accuracy: 0.8267 - val_top-5-accuracy: 0.9904\n",
      "Epoch 161/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.3913 - accuracy: 0.8636 - top-5-accuracy: 0.9959 - val_loss: 0.6214 - val_accuracy: 0.7991 - val_top-5-accuracy: 0.9907\n",
      "Epoch 162/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3949 - accuracy: 0.8638 - top-5-accuracy: 0.9958 - val_loss: 0.5233 - val_accuracy: 0.8213 - val_top-5-accuracy: 0.9930\n",
      "Epoch 163/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3963 - accuracy: 0.8595 - top-5-accuracy: 0.9962 - val_loss: 0.5811 - val_accuracy: 0.8135 - val_top-5-accuracy: 0.9874\n",
      "Epoch 164/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3850 - accuracy: 0.8648 - top-5-accuracy: 0.9958 - val_loss: 0.5081 - val_accuracy: 0.8363 - val_top-5-accuracy: 0.9907\n",
      "Epoch 165/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.4029 - accuracy: 0.8617 - top-5-accuracy: 0.9957 - val_loss: 0.5436 - val_accuracy: 0.8188 - val_top-5-accuracy: 0.9892\n",
      "Epoch 166/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3948 - accuracy: 0.8612 - top-5-accuracy: 0.9963 - val_loss: 0.5588 - val_accuracy: 0.8161 - val_top-5-accuracy: 0.9896\n",
      "Epoch 167/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3969 - accuracy: 0.8615 - top-5-accuracy: 0.9955 - val_loss: 0.5540 - val_accuracy: 0.8149 - val_top-5-accuracy: 0.9907\n",
      "Epoch 168/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.3911 - accuracy: 0.8643 - top-5-accuracy: 0.9962 - val_loss: 0.4974 - val_accuracy: 0.8355 - val_top-5-accuracy: 0.9919\n",
      "Epoch 169/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3843 - accuracy: 0.8670 - top-5-accuracy: 0.9968 - val_loss: 0.5233 - val_accuracy: 0.8305 - val_top-5-accuracy: 0.9908\n",
      "Epoch 170/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3817 - accuracy: 0.8676 - top-5-accuracy: 0.9961 - val_loss: 0.5424 - val_accuracy: 0.8204 - val_top-5-accuracy: 0.9911\n",
      "Epoch 171/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3863 - accuracy: 0.8652 - top-5-accuracy: 0.9965 - val_loss: 0.4954 - val_accuracy: 0.8387 - val_top-5-accuracy: 0.9933\n",
      "Epoch 172/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3889 - accuracy: 0.8629 - top-5-accuracy: 0.9962 - val_loss: 0.6483 - val_accuracy: 0.7899 - val_top-5-accuracy: 0.9861\n",
      "Epoch 173/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3927 - accuracy: 0.8649 - top-5-accuracy: 0.9963 - val_loss: 0.5060 - val_accuracy: 0.8294 - val_top-5-accuracy: 0.9924\n",
      "Epoch 174/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3821 - accuracy: 0.8678 - top-5-accuracy: 0.9963 - val_loss: 0.5665 - val_accuracy: 0.8166 - val_top-5-accuracy: 0.9910\n",
      "Epoch 175/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3857 - accuracy: 0.8669 - top-5-accuracy: 0.9961 - val_loss: 0.5763 - val_accuracy: 0.8110 - val_top-5-accuracy: 0.9898\n",
      "Epoch 176/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3903 - accuracy: 0.8631 - top-5-accuracy: 0.9963 - val_loss: 0.5633 - val_accuracy: 0.8105 - val_top-5-accuracy: 0.9889\n",
      "Epoch 177/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3885 - accuracy: 0.8641 - top-5-accuracy: 0.9957 - val_loss: 0.5232 - val_accuracy: 0.8272 - val_top-5-accuracy: 0.9907\n",
      "Epoch 178/200\n",
      "157/157 [==============================] - 67s 428ms/step - loss: 0.3868 - accuracy: 0.8665 - top-5-accuracy: 0.9961 - val_loss: 0.4546 - val_accuracy: 0.8482 - val_top-5-accuracy: 0.9943\n",
      "Epoch 179/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3896 - accuracy: 0.8642 - top-5-accuracy: 0.9966 - val_loss: 0.6380 - val_accuracy: 0.7958 - val_top-5-accuracy: 0.9860\n",
      "Epoch 180/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3881 - accuracy: 0.8635 - top-5-accuracy: 0.9961 - val_loss: 0.4975 - val_accuracy: 0.8297 - val_top-5-accuracy: 0.9928\n",
      "Epoch 181/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3850 - accuracy: 0.8654 - top-5-accuracy: 0.9966 - val_loss: 0.5647 - val_accuracy: 0.8140 - val_top-5-accuracy: 0.9902\n",
      "Epoch 182/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3881 - accuracy: 0.8656 - top-5-accuracy: 0.9958 - val_loss: 0.4908 - val_accuracy: 0.8399 - val_top-5-accuracy: 0.9944\n",
      "Epoch 183/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3757 - accuracy: 0.8692 - top-5-accuracy: 0.9963 - val_loss: 0.5746 - val_accuracy: 0.8125 - val_top-5-accuracy: 0.9904\n",
      "Epoch 184/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3864 - accuracy: 0.8671 - top-5-accuracy: 0.9965 - val_loss: 0.5614 - val_accuracy: 0.8138 - val_top-5-accuracy: 0.9895\n",
      "Epoch 185/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3791 - accuracy: 0.8666 - top-5-accuracy: 0.9966 - val_loss: 0.5021 - val_accuracy: 0.8355 - val_top-5-accuracy: 0.9911\n",
      "Epoch 186/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3869 - accuracy: 0.8644 - top-5-accuracy: 0.9963 - val_loss: 0.5783 - val_accuracy: 0.8082 - val_top-5-accuracy: 0.9895\n",
      "Epoch 187/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3825 - accuracy: 0.8684 - top-5-accuracy: 0.9963 - val_loss: 0.6113 - val_accuracy: 0.7986 - val_top-5-accuracy: 0.9888\n",
      "Epoch 188/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3861 - accuracy: 0.8671 - top-5-accuracy: 0.9967 - val_loss: 0.6580 - val_accuracy: 0.7878 - val_top-5-accuracy: 0.9886\n",
      "Epoch 189/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3858 - accuracy: 0.8655 - top-5-accuracy: 0.9961 - val_loss: 0.5677 - val_accuracy: 0.8152 - val_top-5-accuracy: 0.9899\n",
      "Epoch 190/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3766 - accuracy: 0.8668 - top-5-accuracy: 0.9969 - val_loss: 0.5206 - val_accuracy: 0.8265 - val_top-5-accuracy: 0.9923\n",
      "Epoch 191/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3758 - accuracy: 0.8672 - top-5-accuracy: 0.9969 - val_loss: 0.5166 - val_accuracy: 0.8299 - val_top-5-accuracy: 0.9903\n",
      "Epoch 192/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3767 - accuracy: 0.8685 - top-5-accuracy: 0.9966 - val_loss: 0.4900 - val_accuracy: 0.8364 - val_top-5-accuracy: 0.9925\n",
      "Epoch 193/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3759 - accuracy: 0.8690 - top-5-accuracy: 0.9967 - val_loss: 0.4691 - val_accuracy: 0.8448 - val_top-5-accuracy: 0.9937\n",
      "Epoch 194/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3824 - accuracy: 0.8659 - top-5-accuracy: 0.9964 - val_loss: 0.6835 - val_accuracy: 0.7823 - val_top-5-accuracy: 0.9810\n",
      "Epoch 195/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3795 - accuracy: 0.8682 - top-5-accuracy: 0.9965 - val_loss: 0.6627 - val_accuracy: 0.7910 - val_top-5-accuracy: 0.9907\n",
      "Epoch 196/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3795 - accuracy: 0.8675 - top-5-accuracy: 0.9963 - val_loss: 0.5053 - val_accuracy: 0.8347 - val_top-5-accuracy: 0.9922\n",
      "Epoch 197/200\n",
      "157/157 [==============================] - 67s 427ms/step - loss: 0.3735 - accuracy: 0.8682 - top-5-accuracy: 0.9967 - val_loss: 0.4630 - val_accuracy: 0.8493 - val_top-5-accuracy: 0.9931\n",
      "Epoch 198/200\n",
      "157/157 [==============================] - 67s 425ms/step - loss: 0.3704 - accuracy: 0.8705 - top-5-accuracy: 0.9966 - val_loss: 0.5504 - val_accuracy: 0.8223 - val_top-5-accuracy: 0.9896\n",
      "Epoch 199/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3791 - accuracy: 0.8656 - top-5-accuracy: 0.9965 - val_loss: 0.5677 - val_accuracy: 0.8201 - val_top-5-accuracy: 0.9884\n",
      "Epoch 200/200\n",
      "157/157 [==============================] - 67s 426ms/step - loss: 0.3787 - accuracy: 0.8683 - top-5-accuracy: 0.9970 - val_loss: 0.5282 - val_accuracy: 0.8243 - val_top-5-accuracy: 0.9920\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.4993 - accuracy: 0.8371 - top-5-accuracy: 0.9927\n",
      "Test accuracy: 83.71%\n",
      "Test top 5 accuracy: 99.27%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay = weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vitc_classifier = create_vitc_classifier()\n",
    "history = run_experiment(vitc_classifier)\n",
    "json.dump(history.history, open(\"vitc_1gf_cifar10_200epoch_tensorflow_2x2\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T22:17:42.383462Z",
     "iopub.status.busy": "2022-01-16T22:17:42.382392Z",
     "iopub.status.idle": "2022-01-16T22:17:42.403804Z",
     "shell.execute_reply": "2022-01-16T22:17:42.402649Z",
     "shell.execute_reply.started": "2022-01-16T22:17:42.3834Z"
    }
   },
   "source": [
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T22:17:45.246289Z",
     "iopub.status.busy": "2022-01-16T22:17:45.24537Z",
     "iopub.status.idle": "2022-01-16T23:56:32.395402Z",
     "shell.execute_reply": "2022-01-16T23:56:32.394323Z",
     "shell.execute_reply.started": "2022-01-16T22:17:45.24625Z"
    }
   },
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)\n",
    "json.dump(history.history, open(\"vitc_1gf_cifar10_200epoch_tensorflow\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-74c7db5447c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
