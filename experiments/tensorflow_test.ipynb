{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceffd0b5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.01235,
     "end_time": "2022-01-20T10:16:11.577844",
     "exception": false,
     "start_time": "2022-01-20T10:16:11.565494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image classification with vision transformer using a convolutional stem\n",
    "Taken from an example implementation of ViT by Khalid Salama\n",
    "\n",
    "Source: https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
    "\n",
    "---\n",
    "\n",
    "Computer Vision Project WS2021/22\n",
    "\n",
    "By Maria R. Lily Djami\n",
    "\n",
    "This notebook shows the usage of the ViTc model we implemented in for this project, as well as the ViT model. The models and training method are defined in the same notebook so that the notebook can easily be uploaded to platforms such as Kaggle and be run on GPUs since we do not otherwise own machines with GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555395a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:11.614044Z",
     "iopub.status.busy": "2022-01-20T10:16:11.613263Z",
     "iopub.status.idle": "2022-01-20T10:16:17.036225Z",
     "shell.execute_reply": "2022-01-20T10:16:17.035193Z",
     "shell.execute_reply.started": "2022-01-20T10:10:23.936963Z"
    },
    "papermill": {
     "duration": 5.444947,
     "end_time": "2022-01-20T10:16:17.036402",
     "exception": false,
     "start_time": "2022-01-20T10:16:11.591455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa572c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:17.225231Z",
     "iopub.status.busy": "2022-01-20T10:16:17.222722Z",
     "iopub.status.idle": "2022-01-20T10:16:17.228144Z",
     "shell.execute_reply": "2022-01-20T10:16:17.228598Z",
     "shell.execute_reply.started": "2022-01-20T10:10:29.288477Z"
    },
    "papermill": {
     "duration": 0.179856,
     "end_time": "2022-01-20T10:16:17.228759",
     "exception": false,
     "start_time": "2022-01-20T10:16:17.048903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure that the GPU is being used by Tensorflow\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a2e6a",
   "metadata": {},
   "source": [
    "## Dataset and Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fbbdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:17.259233Z",
     "iopub.status.busy": "2022-01-20T10:16:17.258701Z",
     "iopub.status.idle": "2022-01-20T10:16:25.811832Z",
     "shell.execute_reply": "2022-01-20T10:16:25.812815Z",
     "shell.execute_reply.started": "2022-01-20T10:10:29.445462Z"
    },
    "papermill": {
     "duration": 8.571384,
     "end_time": "2022-01-20T10:16:25.813035",
     "exception": false,
     "start_time": "2022-01-20T10:16:17.241651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382be2c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:25.910038Z",
     "iopub.status.busy": "2022-01-20T10:16:25.909218Z",
     "iopub.status.idle": "2022-01-20T10:16:25.912346Z",
     "shell.execute_reply": "2022-01-20T10:16:25.912926Z",
     "shell.execute_reply.started": "2022-01-20T10:13:02.561264Z"
    },
    "papermill": {
     "duration": 0.056071,
     "end_time": "2022-01-20T10:16:25.913133",
     "exception": false,
     "start_time": "2022-01-20T10:16:25.857062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameter definitions\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "image_size = 72 # original example uses 72, but this is too small to be used with 18GF stem \n",
    "projection_dim = 64\n",
    "num_heads = 3\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  \n",
    "\n",
    "# Size of the transformer layers\n",
    "transformer_layers = 12\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e659d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:26.125359Z",
     "iopub.status.busy": "2022-01-20T10:16:26.124444Z",
     "iopub.status.idle": "2022-01-20T10:16:31.528139Z",
     "shell.execute_reply": "2022-01-20T10:16:31.528674Z",
     "shell.execute_reply.started": "2022-01-20T10:11:10.589277Z"
    },
    "papermill": {
     "duration": 5.464381,
     "end_time": "2022-01-20T10:16:31.528836",
     "exception": false,
     "start_time": "2022-01-20T10:16:26.064455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0c985",
   "metadata": {
    "papermill": {
     "duration": 0.035012,
     "end_time": "2022-01-20T10:16:31.595053",
     "exception": false,
     "start_time": "2022-01-20T10:16:31.560041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f72ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:31.660877Z",
     "iopub.status.busy": "2022-01-20T10:16:31.660032Z",
     "iopub.status.idle": "2022-01-20T10:16:31.661878Z",
     "shell.execute_reply": "2022-01-20T10:16:31.662298Z",
     "shell.execute_reply.started": "2022-01-20T10:11:12.876947Z"
    },
    "papermill": {
     "duration": 0.037332,
     "end_time": "2022-01-20T10:16:31.662434",
     "exception": false,
     "start_time": "2022-01-20T10:16:31.625102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89052ae7",
   "metadata": {},
   "source": [
    "## Build the ViTp model\n",
    "\n",
    "This is the original vision transformer model. Implementation is taken from https://keras.io/examples/vision/image_classification_with_vision_transformer/. \n",
    "\n",
    "Adjustments were made so that the size of the ViTp model can be easily adjusted using a function parameter, similar to how it is with the ViTc model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d15e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:31.886838Z",
     "iopub.status.busy": "2022-01-20T10:16:31.881329Z",
     "iopub.status.idle": "2022-01-20T10:16:31.892520Z",
     "shell.execute_reply": "2022-01-20T10:16:31.892901Z",
     "shell.execute_reply.started": "2022-01-20T10:15:11.459179Z"
    },
    "papermill": {
     "duration": 0.050363,
     "end_time": "2022-01-20T10:16:31.893086",
     "exception": false,
     "start_time": "2022-01-20T10:16:31.842723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "# Patchify operation is implemented as 2 layers,\n",
    "# the Patches layer and the PatchEncoder layer.\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "def create_vit_classifier(size=\"1GF\"):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    ### START OF STEM ###   \n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    ### END OF STEM ###   \n",
    "\n",
    "    if size == \"18GF\":\n",
    "        num_heads = 12\n",
    "    elif size == \"4GF\":\n",
    "        num_heads = 6\n",
    "    elif size == '1GF': \n",
    "        num_heads = 3\n",
    "    else:\n",
    "        print(\"error!\")\n",
    "        return -1\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81d43e",
   "metadata": {
    "papermill": {
     "duration": 0.030248,
     "end_time": "2022-01-20T10:16:31.722463",
     "exception": false,
     "start_time": "2022-01-20T10:16:31.692215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build the ViTc model\n",
    "\n",
    "The ViTc model introduced in [...] replaces the patchify stem with convolutional layers. The model below implements the 1GF, 4GF, and 18GF variants of the ViTc model. They are currently hardcoded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017bc20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:31.811031Z",
     "iopub.status.busy": "2022-01-20T10:16:31.809355Z",
     "iopub.status.idle": "2022-01-20T10:16:31.811638Z",
     "shell.execute_reply": "2022-01-20T10:16:31.812076Z",
     "shell.execute_reply.started": "2022-01-20T10:11:15.559066Z"
    },
    "papermill": {
     "duration": 0.059627,
     "end_time": "2022-01-20T10:16:31.812231",
     "exception": false,
     "start_time": "2022-01-20T10:16:31.752604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vitc_classifier(stem='1GF', kernel=3):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    ####################################################\n",
    "    #   Here is the stem of the vision transformer.    #\n",
    "    #   For ViTc, we want to use convolutional layers  #\n",
    "    #   instead of patchify to get the encoded image   #\n",
    "    ####################################################\n",
    "    \n",
    "    cnn_stem = keras.Sequential()\n",
    "    \n",
    "    if stem == '18GF':\n",
    "        num_heads = 12\n",
    "\n",
    "        cnn_stem.add(layers.Conv2D(64, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(128, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(128, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(256, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(256, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(512, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "    elif stem == '4GF':\n",
    "        num_heads = 6\n",
    "        \n",
    "        cnn_stem.add(layers.Conv2D(48, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(96, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(192, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(384, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "    elif stem == '1GF': \n",
    "        num_heads = 3\n",
    "        \n",
    "        cnn_stem.add(layers.Conv2D(24, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(48, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(96, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "        cnn_stem.add(layers.Conv2D(192, kernel, strides=(2,2)))\n",
    "        cnn_stem.add(layers.BatchNormalization())\n",
    "        cnn_stem.add(layers.ReLU())\n",
    "    else: \n",
    "        print(\"Invalid stem design given!\")\n",
    "        return -1\n",
    "    \n",
    "    cnn_stem.add(layers.Conv2D(projection_dim, 1, strides=(1,1)))    \n",
    "    \n",
    "    encoded_cnn = cnn_stem(augmented)\n",
    "    \n",
    "    ######################################################\n",
    "    #   The code block below is the transformer block.   #\n",
    "    #   This remains the same between ViTp and ViTc.     #\n",
    "    ######################################################\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers-1):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_cnn)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_cnn])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_cnn = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_cnn)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abaef2",
   "metadata": {
    "papermill": {
     "duration": 0.030341,
     "end_time": "2022-01-20T10:16:31.955577",
     "exception": false,
     "start_time": "2022-01-20T10:16:31.925236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93ab27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:32.025313Z",
     "iopub.status.busy": "2022-01-20T10:16:32.024645Z",
     "iopub.status.idle": "2022-01-20T10:16:32.027565Z",
     "shell.execute_reply": "2022-01-20T10:16:32.027125Z",
     "shell.execute_reply.started": "2022-01-20T10:11:45.174722Z"
    },
    "papermill": {
     "duration": 0.041317,
     "end_time": "2022-01-20T10:16:32.027685",
     "exception": false,
     "start_time": "2022-01-20T10:16:31.986368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # this path may need to be changed accordingly.\n",
    "    # sometimes errors occur if the full path is not given.\n",
    "    checkpoint_filepath = \"tmp\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4587197f",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "Run the experiment with ViTp and ViTc. Results of training (training accuracy and loss, as well as validation accuracy and loss) are saved into a json file, which can be reimported back and plotted using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980cffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:16:32.095556Z",
     "iopub.status.busy": "2022-01-20T10:16:32.094756Z",
     "iopub.status.idle": "2022-01-20T10:22:22.783838Z",
     "shell.execute_reply": "2022-01-20T10:22:22.783318Z",
     "shell.execute_reply.started": "2022-01-20T10:15:34.692229Z"
    },
    "papermill": {
     "duration": 350.725033,
     "end_time": "2022-01-20T10:22:22.784024",
     "exception": false,
     "start_time": "2022-01-20T10:16:32.058991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vitc_classifier = create_vitc_classifier(stem='1GF')\n",
    "history = run_experiment(vitc_classifier)\n",
    "json.dump(history.history, open(\"vitc_1gf_cifar10_10epochs.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79de5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-20T10:22:24.915475Z",
     "iopub.status.busy": "2022-01-20T10:22:24.914875Z",
     "iopub.status.idle": "2022-01-20T10:42:15.315654Z",
     "shell.execute_reply": "2022-01-20T10:42:15.315201Z",
     "shell.execute_reply.started": "2022-01-20T10:15:42.825874Z"
    },
    "papermill": {
     "duration": 1191.479223,
     "end_time": "2022-01-20T10:42:15.315782",
     "exception": false,
     "start_time": "2022-01-20T10:22:23.836559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit_classifier = create_vit_classifier(size='1GF')\n",
    "history = run_experiment(vit_classifier)\n",
    "json.dump(history.history, open(\"vitp_1gf_cifar10_10epochs.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1581.919827,
   "end_time": "2022-01-20T10:42:24.927811",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-20T10:16:03.007984",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
