{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# Image classification with vision transformer using / without a convolutional stem\n","Source: https://keras.io/examples/vision/image_classification_with_vision_transformer/#build-the-vit-model\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:53:23.614147Z","iopub.status.busy":"2022-01-27T10:53:23.613847Z","iopub.status.idle":"2022-01-27T10:53:28.595033Z","shell.execute_reply":"2022-01-27T10:53:28.594288Z","shell.execute_reply.started":"2022-01-27T10:53:23.614064Z"},"trusted":true},"outputs":[],"source":["import json\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:53:34.582361Z","iopub.status.busy":"2022-01-27T10:53:34.581673Z","iopub.status.idle":"2022-01-27T10:53:34.588146Z","shell.execute_reply":"2022-01-27T10:53:34.586367Z","shell.execute_reply.started":"2022-01-27T10:53:34.582321Z"},"trusted":true},"outputs":[],"source":["physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n","config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:53:37.416092Z","iopub.status.busy":"2022-01-27T10:53:37.415493Z","iopub.status.idle":"2022-01-27T10:53:55.902416Z","shell.execute_reply":"2022-01-27T10:53:55.901677Z","shell.execute_reply.started":"2022-01-27T10:53:37.416051Z"},"trusted":true},"outputs":[],"source":["import pickle\n","path = '/kaggle/input/miniimagenet/mini-imagenet-cache-train.pkl'\n","with open(path, 'rb') as f:\n","    data = pickle.load(f)\n","\n","x = np.array(data['image_data'])\n","y = np.array([np.array(int(i / 600)) for i in range(38400)])\n","del(data)\n","\n","x_train = np.array(x[0 * 600:0 * 600 + 540])\n","for i in range(1, 64):\n","    x_train = np.concatenate((x_train, x[i * 600:i * 600 + 540]), axis=0)\n","\n","y_train = np.array(y[0 * 600:0 * 600 + 540])\n","for i in range(1, 64):\n","    y_train = np.concatenate((y_train, y[i * 600:i * 600 +540]), axis=0)\n","y_train = np.reshape(y_train, (34560,1))    \n","\n","x_test = np.array(x[0 * 600 + 540:1 * 600])\n","for i in range(1, 64):\n","    x_test = np.concatenate((x_test, x[i * 600 + 540:(i+1) * 600]), axis=0)\n","\n","y_test = np.array(y[0 * 600 + 540:1 * 600])\n","for i in range(1, 64):\n","    y_test = np.concatenate((y_test, y[i * 600+540:(i+1) * 600]), axis=0)\n","y_test = np.reshape(y_test, (3840,1))   \n","\n","del(x)\n","del(y)\n","\n","num_classes = 64\n","input_shape = (84, 84, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:53:55.904282Z","iopub.status.busy":"2022-01-27T10:53:55.90403Z","iopub.status.idle":"2022-01-27T10:53:55.908985Z","shell.execute_reply":"2022-01-27T10:53:55.908245Z","shell.execute_reply.started":"2022-01-27T10:53:55.904244Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 120\n","num_epochs = 100 # 50, 200\n","image_size = 224  \n","projection_dim = 64\n","num_heads = 3\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  \n","\n","# Size of the transformer layers\n","transformer_layers = 12\n","mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"]},{"cell_type":"markdown","metadata":{},"source":["## Use data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:54:02.014069Z","iopub.status.busy":"2022-01-27T10:54:02.013506Z","iopub.status.idle":"2022-01-27T10:54:04.657949Z","shell.execute_reply":"2022-01-27T10:54:04.657158Z","shell.execute_reply.started":"2022-01-27T10:54:02.014029Z"},"trusted":true},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Normalization(),\n","        layers.Resizing(image_size, image_size),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","        layers.RandomZoom(\n","            height_factor=0.2, width_factor=0.2\n","        ),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","# Compute the mean and the variance of the training data for normalization.\n","data_augmentation.layers[0].adapt(x_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Implement multilayer perceptron (MLP)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:54:07.133309Z","iopub.status.busy":"2022-01-27T10:54:07.133056Z","iopub.status.idle":"2022-01-27T10:54:07.137804Z","shell.execute_reply":"2022-01-27T10:54:07.137132Z","shell.execute_reply.started":"2022-01-27T10:54:07.13328Z"},"trusted":true},"outputs":[],"source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x"]},{"cell_type":"markdown","metadata":{},"source":["## Build the ViTC model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:54:10.234569Z","iopub.status.busy":"2022-01-27T10:54:10.234292Z","iopub.status.idle":"2022-01-27T10:54:10.287247Z","shell.execute_reply":"2022-01-27T10:54:10.286502Z","shell.execute_reply.started":"2022-01-27T10:54:10.234538Z"},"trusted":true},"outputs":[],"source":["def create_vitc_classifier(stem='1GF'):\n","    inputs = layers.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","\n","    '''\n","    Here we'd want to use a convolutional layer instead of patches\n","    to get the encoded image\n","    '''\n","    cnn_stem = keras.Sequential()\n","    \n","    if stem == '18GF':\n","        # using filters [64, 128, 128, 256, 256, 512]\n","    \n","        cnn_stem.add(layers.Conv2D(64, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(128, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(128, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(256, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(256, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(512, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","    elif stem == '4GF':\n","        # using filters [48, 96, 192, 384]\n","        \n","        cnn_stem.add(layers.Conv2D(48, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(96, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(192, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(384, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","    elif stem == '1GF': \n","        # using filters [24, 48, 96, 192]\n","        \n","        cnn_stem.add(layers.Conv2D(24, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(48, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(96, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","        cnn_stem.add(layers.Conv2D(192, 3, strides=(2,2)))\n","        cnn_stem.add(layers.BatchNormalization())\n","        cnn_stem.add(layers.ReLU())\n","    else: \n","        print(\"Invalid stem design given!\")\n","        return -1\n","    \n","    cnn_stem.add(layers.Conv2D(projection_dim, 1, strides=(1,1)))    \n","    \n","    encoded_cnn = cnn_stem(augmented)\n","    #encoded_cnn = tf.reshape(encoded_cnn, (-1,192, 64))\n","    \n","    '''\n","    The part below for the transformer block is unchanged\n","    '''\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers-1):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_cnn)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_cnn])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_cnn = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_cnn)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(num_classes)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Build the ViT model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T14:52:28.711179Z","iopub.status.busy":"2022-01-26T14:52:28.710732Z","iopub.status.idle":"2022-01-26T14:52:28.728211Z","shell.execute_reply":"2022-01-26T14:52:28.727464Z","shell.execute_reply.started":"2022-01-26T14:52:28.711127Z"},"trusted":true},"outputs":[],"source":["patch_size = 16  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","\n","class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n","    \n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded\n","    \n","def create_vit_classifier(size=\"1GF\"):\n","    inputs = layers.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","    \n","    if size == \"18GF\":\n","        num_heads = 12\n","    elif size == \"4GF\":\n","        num_heads = 6\n","    elif size == '1GF': \n","        num_heads = 3\n","    else:\n","        print(\"error!\")\n","        return -1\n","    \n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(num_classes)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Compile, train, and evaluate the mode"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:54:14.705778Z","iopub.status.busy":"2022-01-27T10:54:14.705256Z","iopub.status.idle":"2022-01-27T10:54:14.714243Z","shell.execute_reply":"2022-01-27T10:54:14.713531Z","shell.execute_reply.started":"2022-01-27T10:54:14.705738Z"},"trusted":true},"outputs":[],"source":["def run_experiment(model):\n","#     optimizer = tfa.optimizers.AdamW(\n","#         learning_rate=learning_rate, weight_decay=weight_decay\n","#     )\n","    optimizer = tf.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","\n","    checkpoint_filepath = \"/tmp/checkpoint\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_accuracy\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_data=(x_test, y_test),\n","        callbacks=[checkpoint_callback],\n","    )\n","\n","    model.load_weights(checkpoint_filepath)\n","    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    return history\n"]},{"cell_type":"markdown","metadata":{},"source":["The training is performed below, where changing the size of the model (1GF, 4GF, 18GF) is adjusted in the corresponding position of the code."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T10:54:17.438923Z","iopub.status.busy":"2022-01-27T10:54:17.438246Z","iopub.status.idle":"2022-01-27T12:47:47.500136Z","shell.execute_reply":"2022-01-27T12:47:47.499329Z","shell.execute_reply.started":"2022-01-27T10:54:17.438885Z"},"trusted":true},"outputs":[],"source":["vitc_classifier = create_vitc_classifier(stem='1GF')\n","history = run_experiment(vitc_classifier)\n","with open(\"vitc_1gf_mini_imagenet_100epochs.json\", \"w\") as f:\n","    json.dump(history.history, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T14:54:37.994977Z","iopub.status.busy":"2022-01-26T14:54:37.994719Z"},"trusted":true},"outputs":[],"source":["vit_classifier = create_vit_classifier(size='1GF')\n","history = run_experiment(vit_classifier)\n","with open(\"vitp_1gf_mini_imagenet_100epochs.json\", \"w\") as f:\n","    json.dump(history.history, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
